{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_HL_Kim_Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZaSytyU2FB5"
      },
      "source": [
        "# SAI - I 트리케라톱스\n",
        "실습과제\n",
        "\n",
        "- 회귀 문제를 다뤄보겠음\n",
        "- 데이터셋은 여러 개의 feature가 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0CRJh4A1xcL",
        "outputId": "0d2748f2-6819-4409-d211-b41f19f5159d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO8N1WqP2eVs",
        "outputId": "a948e5ef-e39b-4f27-9b8c-fa6efa9410be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLhgqGj2o1_"
      },
      "source": [
        "13가지의 특성이 존재함. 13가지는 서로 평균, 분산이 모두 다른 정보. 안정적인 학습을 위해선 normalization이 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdVg8g-q2mm9"
      },
      "source": [
        "mean = train_data.mean(axis = 0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= staticmethod\n",
        "\n",
        "test_data -=mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxJ092lh3V5d"
      },
      "source": [
        "여기서 주의깊게 봐야 하는 부분은 **train data의 평균과 분산을 test data를 normalization 시킬 때도 동일하게 사용**했다는 점이다.\n",
        "\n",
        "test data를 갖고는 어떠한 연산도 해선 안 되기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQUtPJHB3gWS"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',\n",
        "                        input_shape = (train_data.shape[1], )))\n",
        "  model.add(layers.Dense(64, activation = 'relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  \n",
        "  model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA8ZDjhs4cy0"
      },
      "source": [
        "모델을 여러 번 새로 갈아엎을 것이기 때문에 선언부를 함수로 만들어 준다.\n",
        "훈련 데이터의 개수가 적을 수록 과대적합이 쉽게 일어나는 편이다. 따라서 작은 모델을 쓰는 편이 좋다.\n",
        "- MSE : Mean Square Error. 회귀 문제에서 자주 쓰이는 편이다.\n",
        "- MAE : Mean Absolute Error. 모니터링을 위해 쓰는 지표인데, 예측과 타깃 사이 거리의 절대값을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CxzOiyq5IRb"
      },
      "source": [
        "## K-겹 검증\n",
        "K-fold cross-validation\n",
        "\n",
        "모델을 평가하기 위해서 훈련 세트와 검증 세트로 나누어야 하는데, 데이터가 적어서 검증 세트도 작아진다. 그래서 검증 세트를 어떻게 나누었느냐에 따라 성능이 크게 달라지기도 한다. 이를 막기 위해 있는 방법이 K겹 검증임.\n",
        "\n",
        "쉽게 말하면 그냥 K조각 만큼 데이터를 분할하고 한 조각을 평가 세트로, 나머지 조각들을 훈련 세트로 삼는 것이다. 그리고 촤종 점수는 그렇게 한 조각씩 선택해서 얻은 검증 점수의 평균을 취한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc_xDFLu4Zq6"
      },
      "source": [
        "# from sklearn.model_selection import KFold # 교재에선 그냥 생으로 구현했는데, 한 번 외부 패키지 찾아 봤다."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZMFyy066o9o",
        "outputId": "563ea035-f041-41f9-8f6f-edb337e1e0a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "  print(\"처리중인 폴드: {}\".format(i))\n",
        "  val_data = train_data[ i * num_val_samples: (i+1) * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i*num_val_samples],\n",
        "       train_data[(i+1)*num_val_samples:]],\n",
        "       axis = 0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets[:i*num_val_samples],\n",
        "       train_targets[(i+1)*num_val_samples:]],\n",
        "       axis = 0)\n",
        "  \n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs = num_epochs, batch_size = 1, verbose = 0) # verbose = 0 이면 훈련 과정이 출력되지 않음\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0 )\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리중인 폴드: 0\n",
            "처리중인 폴드: 1\n",
            "처리중인 폴드: 2\n",
            "처리중인 폴드: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb7PQUO48gZq",
        "outputId": "11f18687-db2c-4032-9a42-35a9090c9bf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(all_scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.24340558052063, 2.8697080612182617, 3.2149622440338135, 3.5428271293640137]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCyoSM9-_NQ-",
        "outputId": "fa2878c9-cda4-4e33-e7a2-109bbbcb468a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.mean(all_scores)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2177257537841797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeikhLd4_sI9"
      },
      "source": [
        "그 다음 내용은.. 솔직히 무슨 내용인지 잘 모르겠음. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZV9N90P_Qus"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}